[TOC]

本文介绍了调试spider的最常用技术。请考虑下面的蜘蛛：

```
import scrapy
from myproject.items import MyItem

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = (
        'http://example.com/page1',
        'http://example.com/page2',
        )

    def parse(self, response):
        # <processing code not shown>
        # collect `item_urls`
        for item_url in item_urls:
            yield scrapy.Request(item_url, self.parse_item)

    def parse_item(self, response):
        # <processing code not shown>
        item = MyItem()
        # populate `item` fields
        # and extract item_details_url
        yield scrapy.Request(item_details_url, self.parse_details, cb_kwargs={'item': item})

    def parse_details(self, response, item):
        # populate more `item` fields
        return item
```

基本上，这是一个简单的spider，它解析两页项目（start-url）。项目还有一个包含附加信息的详细信息页，因此我们使用 `cb_kwargs` 的功能 [`Request`](https://www.osgeo.cn/scrapy/topics/request-response.html#scrapy.http.Request) 传递部分填充的项。

## 解析命令

检查蜘蛛输出的最基本方法是使用 [`parse`](https://www.osgeo.cn/scrapy/topics/commands.html#std-command-parse) 命令。它允许在方法级别检查spider的不同部分的行为。它的优点是灵活和易于使用，但不允许在方法内部调试代码。

为了查看从特定URL中获取的项目：

```
$ scrapy parse --spider=myspider -c parse_item -d 2 <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 2 <<<
# Scraped Items  ------------------------------------------------------------
[{'url': <item_url>}]

# Requests  -----------------------------------------------------------------
[]
```

使用 `--verbose` 或 `-v` 选项我们可以看到每个深度级别的状态：

```
$ scrapy parse --spider=myspider -c parse_item -d 2 -v <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> DEPTH LEVEL: 1 <<<
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[<GET item_details_url>]


>>> DEPTH LEVEL: 2 <<<
# Scraped Items  ------------------------------------------------------------
[{'url': <item_url>}]

# Requests  -----------------------------------------------------------------
[]
```

检查从一个开始的项目，也可以很容易地实现使用：：

```
$ scrapy parse --spider=myspider -d 3 'http://example.com/page1'
```

## Scrapy Shell

而 [`parse`](https://www.osgeo.cn/scrapy/topics/commands.html#std-command-parse) 命令对于检查蜘蛛的行为非常有用，除了显示接收到的响应和输出之外，检查回调中发生的情况几乎没有帮助。如何调试情况 `parse_details` 有时没有收到物品？

幸运的是， [`shell`](https://www.osgeo.cn/scrapy/topics/commands.html#std-command-shell) 在这种情况下，你的面包和黄油（见 [从spiders调用shell来检查响应](https://www.osgeo.cn/scrapy/topics/shell.html#topics-shell-inspect-response) ）：

```
from scrapy.shell import inspect_response

def parse_details(self, response, item=None):
    if item:
        # populate more `item` fields
        return item
    else:
        inspect_response(response, self)
```

参见： [从spiders调用shell来检查响应](https://www.osgeo.cn/scrapy/topics/shell.html#topics-shell-inspect-response) .

## 在浏览器中打开

有时，您只想查看某个响应在浏览器中的外观，可以使用 `open_in_browser` 功能。以下是您将如何使用它的示例：

```
from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if "item name" not in response.body:
        open_in_browser(response)
```

`open_in_browser` 将打开一个浏览器，此时Scrapy接收到响应，调整 [base tag](https://www.w3schools.com/tags/tag_base.asp) 以便正确显示图像和样式。

## 日志

日志记录是获取蜘蛛运行信息的另一个有用选项。尽管不太方便，但它的优点是，如果需要，日志在将来的所有运行中都将可用：

```
def parse_details(self, response, item=None):
    if item:
        # populate more `item` fields
        return item
    else:
        self.logger.warning('No item received for %s', response.url)
```